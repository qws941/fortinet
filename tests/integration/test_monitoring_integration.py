#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ëª¨ë‹ˆí„°ë§ ë° ì‹¤ì‹œê°„ ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸
ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§, ì‹¤ì‹œê°„ ì•Œë¦¼, ì„±ëŠ¥ ì¶”ì  ê¸°ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.

í…ŒìŠ¤íŠ¸ ë²”ìœ„:
- ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼
- ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°
- ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì—…ë°ì´íŠ¸
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ì´ë²¤íŠ¸ ìƒê´€ê´€ê³„ ë¶„ì„
"""

import asyncio
import json
import os
import random
import sys
import threading
import time
from datetime import datetime, timedelta
from pathlib import Path
from queue import Queue
from typing import Any, Dict, List
from unittest.mock import MagicMock, Mock, patch

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.utils.integration_test_framework import test_framework

# Import actual classes when they exist
try:
    from src.monitoring.monitoring_manager import MonitoringManager
except ImportError:
    MonitoringManager = None

try:
    from src.monitoring.metrics_collector import MetricsCollector
except ImportError:
    MetricsCollector = None

try:
    from src.monitoring.alert_engine import AlertEngine
except ImportError:
    AlertEngine = None

try:
    from src.monitoring.event_correlator import EventCorrelator
except ImportError:
    EventCorrelator = None

try:
    from src.monitoring.performance_monitor import PerformanceMonitor
except ImportError:
    PerformanceMonitor = None

try:
    from src.utils.log_manager import LogManager
except ImportError:
    LogManager = None


# =============================================================================
# ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
# =============================================================================


@test_framework.test("monitoring_realtime_metrics_collection")
def test_realtime_metrics_collection():
    """ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""

    if MetricsCollector is None:
        test_framework.assert_ok(True, "MetricsCollector not available - skipping test")
        return

    metrics_collector = MetricsCollector()

    # 1. CPU/ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    with patch.object(metrics_collector, "collect_system_metrics") as mock_collect:
        mock_collect.return_value = {
            "timestamp": datetime.now().isoformat(),
            "cpu": {
                "usage_percent": 45.2,
                "core_count": 8,
                "load_average": [1.5, 1.2, 0.9],
            },
            "memory": {
                "total_gb": 32,
                "used_gb": 18.5,
                "percent": 57.8,
                "available_gb": 13.5,
            },
            "disk": {
                "total_gb": 500,
                "used_gb": 380,
                "percent": 76.0,
                "io_read_mb": 125.5,
                "io_write_mb": 89.3,
            },
        }

        metrics = metrics_collector.collect_system_metrics()

        test_framework.assert_ok("cpu" in metrics, "Should collect CPU metrics")
        test_framework.assert_ok(
            metrics.get("memory", {}).get("percent", 0) < 80,
            "Memory usage should be reasonable",
        )
        test_framework.assert_ok(
            metrics.get("disk", {}).get("percent", 0) < 90,
            "Disk usage should not be critical",
        )

    # 2. ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    with patch.object(metrics_collector, "collect_network_metrics") as mock_network:
        mock_network.return_value = {
            "interfaces": [
                {
                    "name": "eth0",
                    "bytes_sent": 1024000000,
                    "bytes_recv": 2048000000,
                    "packets_sent": 1000000,
                    "packets_recv": 1500000,
                    "errors_in": 0,
                    "errors_out": 0,
                    "dropped": 0,
                }
            ],
            "connections": {"tcp": 150, "udp": 50, "established": 120, "time_wait": 30},
            "bandwidth": {"inbound_mbps": 85.5, "outbound_mbps": 45.2},
        }

        network_metrics = metrics_collector.collect_network_metrics()

        test_framework.assert_ok(
            len(network_metrics.get("interfaces", [])) > 0,
            "Should have network interfaces",
        )
        test_framework.assert_ok(
            network_metrics.get("connections", {}).get("tcp", 0) > 0,
            "Should have active TCP connections",
        )

    # 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    with patch.object(metrics_collector, "collect_app_metrics") as mock_app:
        mock_app.return_value = {
            "api_requests": {
                "total": 10000,
                "success": 9850,
                "errors": 150,
                "avg_response_time_ms": 125,
            },
            "active_sessions": 85,
            "queue_sizes": {"task_queue": 12, "event_queue": 5, "alert_queue": 0},
            "cache_stats": {"hits": 8500, "misses": 1500, "hit_rate": 0.85},
        }

        app_metrics = metrics_collector.collect_app_metrics()

        test_framework.assert_ok(
            app_metrics.get("api_requests", {}).get("success", 0) > 0,
            "Should track API success",
        )
        test_framework.assert_ok(
            app_metrics.get("cache_stats", {}).get("hit_rate", 0) > 0.8,
            "Cache hit rate should be good",
        )


@test_framework.test("monitoring_metrics_aggregation_window")
def test_metrics_aggregation_and_windowing():
    """ë©”íŠ¸ë¦­ ì§‘ê³„ ë° ì‹œê°„ ìœˆë„ìš° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""

    metrics_collector = MetricsCollector()

    # 5ë¶„ ë™ì•ˆì˜ ë©”íŠ¸ë¦­ ì‹œë®¬ë ˆì´ì…˜
    time_series_data = []
    for i in range(30):  # 10ì´ˆ ê°„ê²©ìœ¼ë¡œ 30ê°œ = 5ë¶„
        time_series_data.append(
            {
                "timestamp": datetime.now() - timedelta(seconds=(30 - i) * 10),
                "cpu_usage": 40 + random.randint(-10, 20),
                "memory_usage": 60 + random.randint(-5, 10),
                "requests_per_second": 100 + random.randint(-20, 50),
            }
        )

    # ì§‘ê³„ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    with patch.object(metrics_collector, "aggregate_metrics") as mock_aggregate:
        mock_aggregate.return_value = {
            "window": "5_minutes",
            "cpu": {"avg": 45.5, "min": 32, "max": 68, "p95": 62},
            "memory": {"avg": 62.3, "min": 55, "max": 72, "p95": 70},
            "requests": {"total": 36000, "avg_per_second": 120, "peak_per_second": 150},
        }

        aggregated = metrics_collector.aggregate_metrics(time_series_data, window="5_minutes")

        test_framework.assert_ok(aggregated.get("cpu", {}).get("avg", 0) > 0, "Should calculate average CPU")
        test_framework.assert_ok(
            aggregated.get("cpu", {}).get("max", 0) > aggregated.get("cpu", {}).get("avg", 0),
            "Max should be greater than average",
        )


# =============================================================================
# ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼ í…ŒìŠ¤íŠ¸
# =============================================================================


@test_framework.test("monitoring_threshold_based_alerts")
def test_threshold_based_alerting():
    """ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""

    alert_engine = AlertEngine()

    # ì•Œë¦¼ ê·œì¹™ ì •ì˜
    alert_rules = [
        {
            "id": "cpu_high",
            "metric": "cpu_usage",
            "condition": "greater_than",
            "threshold": 80,
            "duration": 300,  # 5ë¶„ ì§€ì†
            "severity": "warning",
        },
        {
            "id": "memory_critical",
            "metric": "memory_percent",
            "condition": "greater_than",
            "threshold": 90,
            "duration": 60,  # 1ë¶„ ì§€ì†
            "severity": "critical",
        },
        {
            "id": "disk_space_low",
            "metric": "disk_free_gb",
            "condition": "less_than",
            "threshold": 10,
            "duration": 0,  # ì¦‰ì‹œ
            "severity": "warning",
        },
    ]

    # 1. ì •ìƒ ë©”íŠ¸ë¦­ - ì•Œë¦¼ ì—†ìŒ
    normal_metrics = {"cpu_usage": 45, "memory_percent": 65, "disk_free_gb": 120}

    with patch.object(alert_engine, "check_alerts") as mock_check:
        mock_check.return_value = {
            "alerts_triggered": [],
            "alerts_cleared": [],
            "active_alerts": 0,
        }

        result = alert_engine.check_alerts(normal_metrics, alert_rules)

        test_framework.assert_eq(
            len(result.get("alerts_triggered", [])),
            0,
            "Normal metrics should not trigger alerts",
        )

    # 2. ì„ê³„ê°’ ì´ˆê³¼ - ì•Œë¦¼ ë°œìƒ
    high_metrics = {"cpu_usage": 95, "memory_percent": 92, "disk_free_gb": 8}

    with patch.object(alert_engine, "check_alerts") as mock_check:
        mock_check.return_value = {
            "alerts_triggered": [
                {
                    "rule_id": "cpu_high",
                    "severity": "warning",
                    "message": "CPU usage at 95% (threshold: 80%)",
                    "timestamp": datetime.now().isoformat(),
                },
                {
                    "rule_id": "memory_critical",
                    "severity": "critical",
                    "message": "Memory usage critical at 92%",
                    "timestamp": datetime.now().isoformat(),
                },
                {
                    "rule_id": "disk_space_low",
                    "severity": "warning",
                    "message": "Low disk space: 8GB remaining",
                    "timestamp": datetime.now().isoformat(),
                },
            ],
            "alerts_cleared": [],
            "active_alerts": 3,
        }

        alerts = alert_engine.check_alerts(high_metrics, alert_rules)

        test_framework.assert_eq(len(alerts.get("alerts_triggered", [])), 3, "Should trigger 3 alerts")
        test_framework.assert_ok(
            any(a["severity"] == "critical" for a in alerts.get("alerts_triggered", [])),
            "Should have critical alert",
        )

    # 3. ì•Œë¦¼ ì—ìŠ¤ì»¬ë ˆì´ì…˜
    with patch.object(alert_engine, "escalate_alert") as mock_escalate:
        mock_escalate.return_value = {
            "escalated": True,
            "notification_sent": ["email", "sms", "slack"],
            "oncall_paged": True,
            "escalation_level": 2,
        }

        critical_alert = {
            "rule_id": "memory_critical",
            "severity": "critical",
            "duration_minutes": 15,
        }

        escalation = alert_engine.escalate_alert(critical_alert)

        test_framework.assert_ok(escalation.get("escalated"), "Critical alert should escalate")
        test_framework.assert_ok(escalation.get("oncall_paged"), "Should page on-call for critical alerts")


@test_framework.test("monitoring_alert_suppression_dedup")
def test_alert_suppression_and_deduplication():
    """ì•Œë¦¼ ì–µì œ ë° ì¤‘ë³µ ì œê±° í…ŒìŠ¤íŠ¸"""

    alert_engine = AlertEngine()

    # ë°˜ë³µì ì¸ ì•Œë¦¼ ì‹œë®¬ë ˆì´ì…˜
    repeated_alerts = []
    for i in range(10):
        repeated_alerts.append(
            {
                "rule_id": "api_errors_high",
                "metric_value": 25 + i,
                "timestamp": datetime.now() + timedelta(seconds=i * 30),
            }
        )

    with patch.object(alert_engine, "process_alerts_with_suppression") as mock_suppress:
        mock_suppress.return_value = {
            "total_alerts": 10,
            "suppressed": 8,
            "sent": 2,
            "suppression_reason": "Rate limit: 1 alert per 5 minutes",
            "next_alert_allowed": datetime.now() + timedelta(minutes=5),
        }

        result = alert_engine.process_alerts_with_suppression(repeated_alerts)

        test_framework.assert_ok(
            result.get("suppressed", 0) > result.get("sent", 0),
            "Should suppress most repeated alerts",
        )
        test_framework.assert_eq(result.get("sent"), 2, "Should send limited alerts")


# =============================================================================
# ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
# =============================================================================


@test_framework.test("monitoring_log_streaming_realtime")
def test_realtime_log_streaming():
    """ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸"""

    log_manager = LogManager()

    # ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜
    log_queue = Queue()

    def log_producer():
        """ë¡œê·¸ ìƒì„±ê¸°"""
        log_types = ["access", "error", "security", "audit"]
        severity_levels = ["info", "warning", "error", "critical"]

        for i in range(50):
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "type": random.choice(log_types),
                "severity": random.choice(severity_levels),
                "source": f"system-{random.randint(1, 5)}",
                "message": f"Test log entry {i}",
                "metadata": {
                    "request_id": f"req-{i}",
                    "user": f"user{random.randint(1, 10)}",
                },
            }
            log_queue.put(log_entry)
            time.sleep(0.01)  # 10ms ê°„ê²©

    # ë¡œê·¸ í”„ë¡œë“€ì„œ ìŠ¤ë ˆë“œ ì‹œì‘
    producer_thread = threading.Thread(target=log_producer)
    producer_thread.daemon = True
    producer_thread.start()

    # ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
    streamed_logs = []
    start_time = time.time()

    while time.time() - start_time < 1:  # 1ì´ˆ ë™ì•ˆ ìŠ¤íŠ¸ë¦¬ë°
        if not log_queue.empty():
            log = log_queue.get()
            streamed_logs.append(log)

    test_framework.assert_ok(
        len(streamed_logs) > 40,
        f"Should stream most logs in real-time ({len(streamed_logs)}/50)",
    )

    # ë¡œê·¸ í•„í„°ë§ í…ŒìŠ¤íŠ¸
    error_logs = [l for l in streamed_logs if l.get("severity") in ["error", "critical"]]
    test_framework.assert_ok(len(error_logs) > 0, "Should have some error/critical logs")


@test_framework.test("monitoring_log_analysis_patterns")
def test_log_pattern_analysis():
    """ë¡œê·¸ íŒ¨í„´ ë¶„ì„ ë° ì´ìƒ íƒì§€"""

    log_manager = LogManager()

    # ì •ìƒ ë° ë¹„ì •ìƒ ë¡œê·¸ íŒ¨í„´
    log_samples = [
        # ì •ìƒ íŒ¨í„´
        {"pattern": "User login successful", "count": 100, "anomaly": False},
        {"pattern": "API request completed", "count": 500, "anomaly": False},
        # ë¹„ì •ìƒ íŒ¨í„´
        {"pattern": "Authentication failed", "count": 50, "anomaly": True},
        {"pattern": "Database connection timeout", "count": 10, "anomaly": True},
        # ê¸‰ì¦ íŒ¨í„´
        {"pattern": "404 Not Found", "count": 1000, "anomaly": True},
    ]

    with patch.object(log_manager, "analyze_patterns") as mock_analyze:
        mock_analyze.return_value = {
            "patterns_detected": 5,
            "anomalies": [
                {
                    "pattern": "Authentication failed",
                    "severity": "high",
                    "rate_increase": 500,  # 500% ì¦ê°€
                    "recommendation": "Check for brute force attack",
                },
                {
                    "pattern": "404 Not Found",
                    "severity": "medium",
                    "rate_increase": 1000,
                    "recommendation": "Possible scanning activity",
                },
            ],
            "trends": {
                "error_rate": "increasing",
                "request_volume": "stable",
                "response_time": "degrading",
            },
        }

        analysis = log_manager.analyze_patterns(log_samples)

        test_framework.assert_ok(len(analysis.get("anomalies", [])) > 0, "Should detect anomalies")
        test_framework.assert_ok(
            any(a["severity"] == "high" for a in analysis.get("anomalies", [])),
            "Should detect high severity anomaly",
        )


# =============================================================================
# ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
# =============================================================================


@test_framework.test("monitoring_dashboard_data_realtime")
def test_dashboard_realtime_updates():
    """ëŒ€ì‹œë³´ë“œ ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸"""

    monitoring_manager = MonitoringManager()

    # ëŒ€ì‹œë³´ë“œ ìœ„ì ¯ ë°ì´í„°
    with patch.object(monitoring_manager, "get_dashboard_data") as mock_dashboard:
        mock_dashboard.return_value = {
            "widgets": {
                "system_health": {
                    "status": "healthy",
                    "score": 95,
                    "last_updated": datetime.now().isoformat(),
                },
                "traffic_overview": {
                    "inbound_mbps": 125.5,
                    "outbound_mbps": 89.3,
                    "active_connections": 1250,
                    "blocked_attempts": 23,
                },
                "top_threats": [
                    {"ip": "203.0.113.10", "attempts": 150, "type": "brute_force"},
                    {"ip": "203.0.113.20", "attempts": 89, "type": "port_scan"},
                ],
                "service_status": {
                    "api": "operational",
                    "database": "operational",
                    "cache": "degraded",
                    "monitoring": "operational",
                },
            },
            "refresh_interval": 5,  # 5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
        }

        dashboard_data = monitoring_manager.get_dashboard_data()

        test_framework.assert_ok(
            dashboard_data.get("widgets", {}).get("system_health", {}).get("score", 0) > 90,
            "System should be healthy",
        )
        test_framework.assert_ok(
            "top_threats" in dashboard_data.get("widgets", {}),
            "Should include threat data",
        )

    # ì‹œê³„ì—´ ì°¨íŠ¸ ë°ì´í„°
    with patch.object(monitoring_manager, "get_timeseries_data") as mock_timeseries:
        mock_timeseries.return_value = {
            "cpu_usage": {
                "labels": [f"{i}:00" for i in range(24)],
                "data": [45 + random.randint(-10, 15) for _ in range(24)],
            },
            "request_rate": {
                "labels": [f"{i}:00" for i in range(24)],
                "data": [1000 + random.randint(-200, 500) for _ in range(24)],
            },
            "error_rate": {
                "labels": [f"{i}:00" for i in range(24)],
                "data": [2 + random.randint(0, 5) for _ in range(24)],
            },
        }

        timeseries = monitoring_manager.get_timeseries_data("24_hours")

        test_framework.assert_eq(
            len(timeseries.get("cpu_usage", {}).get("data", [])),
            24,
            "Should have 24 hours of data",
        )


@test_framework.test("monitoring_dashboard_websocket_updates")
def test_dashboard_websocket_streaming():
    """WebSocketì„ í†µí•œ ëŒ€ì‹œë³´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸"""

    monitoring_manager = MonitoringManager()

    # WebSocket ì´ë²¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜
    update_events = []

    def simulate_updates():
        for i in range(10):
            event = {
                "type": "metric_update",
                "timestamp": datetime.now().isoformat(),
                "data": {
                    "cpu_usage": 40 + random.randint(-5, 10),
                    "active_users": 100 + random.randint(-10, 20),
                    "api_latency_ms": 50 + random.randint(-10, 30),
                },
            }
            update_events.append(event)
            time.sleep(0.1)

    # ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ ì‹¤í–‰
    update_thread = threading.Thread(target=simulate_updates)
    update_thread.start()
    update_thread.join(timeout=2)

    test_framework.assert_ok(
        len(update_events) >= 8,
        f"Should receive most updates ({len(update_events)}/10)",
    )

    # ì—…ë°ì´íŠ¸ ë¹ˆë„ í™•ì¸
    if len(update_events) >= 2:
        time_diff = (
            datetime.fromisoformat(update_events[1]["timestamp"])
            - datetime.fromisoformat(update_events[0]["timestamp"])
        ).total_seconds()

        test_framework.assert_ok(time_diff < 0.2, "Updates should be frequent")  # 200ms ì´ë‚´


# =============================================================================
# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸
# =============================================================================


@test_framework.test("monitoring_api_performance_tracking")
def test_api_performance_monitoring():
    """API ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì¶”ì """

    performance_monitor = PerformanceMonitor()

    # API ì—”ë“œí¬ì¸íŠ¸ë³„ ì„±ëŠ¥ ë°ì´í„°
    api_metrics = {
        "/api/fortigate/policies": {
            "count": 1000,
            "avg_response_ms": 125,
            "p95_response_ms": 250,
            "p99_response_ms": 500,
            "errors": 5,
        },
        "/api/fortimanager/devices": {
            "count": 500,
            "avg_response_ms": 200,
            "p95_response_ms": 400,
            "p99_response_ms": 800,
            "errors": 2,
        },
        "/api/logs/search": {
            "count": 200,
            "avg_response_ms": 1500,
            "p95_response_ms": 3000,
            "p99_response_ms": 5000,
            "errors": 10,
        },
    }

    with patch.object(performance_monitor, "analyze_performance") as mock_analyze:
        mock_analyze.return_value = {
            "slowest_endpoints": [
                {
                    "endpoint": "/api/logs/search",
                    "avg_response_ms": 1500,
                    "recommendation": "Consider adding pagination or caching",
                }
            ],
            "error_prone_endpoints": [
                {
                    "endpoint": "/api/logs/search",
                    "error_rate": 0.05,
                    "recommendation": "Investigate timeout issues",
                }
            ],
            "sla_compliance": {
                "target_p95_ms": 1000,
                "compliant_endpoints": 2,
                "non_compliant_endpoints": 1,
            },
            "overall_health": "degraded",
        }

        analysis = performance_monitor.analyze_performance(api_metrics)

        test_framework.assert_ok(
            len(analysis.get("slowest_endpoints", [])) > 0,
            "Should identify slow endpoints",
        )
        test_framework.assert_ok(
            analysis.get("sla_compliance", {}).get("non_compliant_endpoints", 0) > 0,
            "Should detect SLA violations",
        )


@test_framework.test("monitoring_resource_usage_trends")
def test_resource_usage_trend_analysis():
    """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¶”ì„¸ ë¶„ì„"""

    performance_monitor = PerformanceMonitor()

    # ì¼ì£¼ì¼ ê°„ì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ë°ì´í„°
    weekly_data = []
    for day in range(7):
        for hour in range(24):
            weekly_data.append(
                {
                    "timestamp": datetime.now() - timedelta(days=day, hours=hour),
                    "cpu_percent": 30 + (10 if 9 <= hour <= 17 else 0) + random.randint(-5, 5),
                    "memory_percent": 50 + (15 if 9 <= hour <= 17 else 0) + random.randint(-5, 5),
                    "disk_io_mbps": 20 + (30 if 9 <= hour <= 17 else 0) + random.randint(-10, 10),
                }
            )

    with patch.object(performance_monitor, "analyze_trends") as mock_trends:
        mock_trends.return_value = {
            "patterns": {
                "daily_peak_hours": [9, 10, 11, 14, 15, 16],
                "weekly_peak_days": ["Monday", "Tuesday", "Wednesday"],
                "resource_correlation": "CPU and memory usage highly correlated",
            },
            "predictions": {
                "next_peak": "Tomorrow 10:00 AM",
                "capacity_warning": "Memory may exceed 80% during next peak",
                "recommended_action": "Consider scaling up memory by 8GB",
            },
            "anomalies": [
                {
                    "timestamp": "2024-07-23 03:00",
                    "type": "unusual_cpu_spike",
                    "value": 85,
                    "expected": 35,
                }
            ],
        }

        trend_analysis = performance_monitor.analyze_trends(weekly_data)

        test_framework.assert_ok(
            len(trend_analysis.get("patterns", {}).get("daily_peak_hours", [])) > 0,
            "Should identify peak hours",
        )
        test_framework.assert_ok(
            "capacity_warning" in trend_analysis.get("predictions", {}),
            "Should predict capacity issues",
        )


# =============================================================================
# ì´ë²¤íŠ¸ ìƒê´€ê´€ê³„ ë¶„ì„ í…ŒìŠ¤íŠ¸
# =============================================================================


@test_framework.test("monitoring_event_correlation_analysis")
def test_event_correlation_and_root_cause():
    """ì´ë²¤íŠ¸ ìƒê´€ê´€ê³„ ë¶„ì„ ë° ê·¼ë³¸ ì›ì¸ íŒŒì•…"""

    event_correlator = EventCorrelator()

    # ê´€ë ¨ëœ ì´ë²¤íŠ¸ë“¤
    events = [
        {
            "timestamp": datetime.now() - timedelta(minutes=5),
            "type": "database_slow_query",
            "severity": "warning",
            "details": {"query_time_ms": 5000},
        },
        {
            "timestamp": datetime.now() - timedelta(minutes=4),
            "type": "api_timeout",
            "severity": "error",
            "details": {"endpoint": "/api/reports", "timeout_ms": 30000},
        },
        {
            "timestamp": datetime.now() - timedelta(minutes=3),
            "type": "cache_miss_spike",
            "severity": "warning",
            "details": {"miss_rate": 0.45},
        },
        {
            "timestamp": datetime.now() - timedelta(minutes=2),
            "type": "user_complaints",
            "severity": "high",
            "details": {"count": 15, "issue": "slow_loading"},
        },
    ]

    with patch.object(event_correlator, "correlate_events") as mock_correlate:
        mock_correlate.return_value = {
            "correlation_found": True,
            "root_cause": {
                "type": "database_performance_degradation",
                "confidence": 0.85,
                "evidence": [
                    "Slow queries started 5 minutes ago",
                    "API timeouts followed database issues",
                    "Cache misses increased due to timeout fallbacks",
                    "User complaints align with performance degradation",
                ],
            },
            "impact_analysis": {
                "affected_services": ["api", "web", "reports"],
                "affected_users": 150,
                "business_impact": "high",
            },
            "recommended_actions": [
                {
                    "priority": 1,
                    "action": "Analyze and optimize slow database queries",
                    "expected_result": "Restore normal response times",
                },
                {
                    "priority": 2,
                    "action": "Increase database connection pool size",
                    "expected_result": "Handle spike in concurrent queries",
                },
                {
                    "priority": 3,
                    "action": "Warm up cache after resolution",
                    "expected_result": "Restore cache hit rate",
                },
            ],
        }

        correlation = event_correlator.correlate_events(events)

        test_framework.assert_ok(correlation.get("correlation_found"), "Should find event correlation")
        test_framework.assert_ok(
            correlation.get("root_cause", {}).get("confidence", 0) > 0.8,
            "Should have high confidence in root cause",
        )
        test_framework.assert_ok(
            len(correlation.get("recommended_actions", [])) > 0,
            "Should provide actionable recommendations",
        )


@test_framework.test("monitoring_predictive_alerting")
def test_predictive_monitoring_and_alerting():
    """ì˜ˆì¸¡ì  ëª¨ë‹ˆí„°ë§ ë° ì‚¬ì „ ì•Œë¦¼"""

    monitoring_manager = MonitoringManager()

    # ê³¼ê±° íŒ¨í„´ ë°ì´í„°
    historical_patterns = {
        "daily_peaks": {
            "10:00": {"cpu": 75, "memory": 80},
            "14:00": {"cpu": 70, "memory": 75},
            "16:00": {"cpu": 65, "memory": 70},
        },
        "weekly_patterns": {"Monday": {"avg_load": 80}, "Friday": {"avg_load": 90}},
        "monthly_events": {"month_end": {"load_spike": 1.5}},  # 150% of normal
    }

    with patch.object(monitoring_manager, "predict_issues") as mock_predict:
        mock_predict.return_value = {
            "predictions": [
                {
                    "issue": "Memory exhaustion likely",
                    "probability": 0.75,
                    "expected_time": "Tomorrow 10:15 AM",
                    "current_trend": "Memory usage increasing 5% per hour",
                    "preventive_action": "Restart memory-intensive services tonight",
                },
                {
                    "issue": "Disk space critical",
                    "probability": 0.90,
                    "expected_time": "In 3 days",
                    "current_trend": "15GB consumed daily",
                    "preventive_action": "Archive old logs and cleanup temp files",
                },
            ],
            "confidence_level": "high",
            "based_on": "7 days of historical data",
        }

        predictions = monitoring_manager.predict_issues(historical_patterns)

        test_framework.assert_ok(len(predictions.get("predictions", [])) > 0, "Should make predictions")
        test_framework.assert_ok(
            any(p["probability"] > 0.8 for p in predictions.get("predictions", [])),
            "Should have high-probability predictions",
        )


# =============================================================================
# í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œë‚˜ë¦¬ì˜¤
# =============================================================================


@test_framework.test("monitoring_full_stack_scenario")
def test_full_stack_monitoring_scenario():
    """ì „ì²´ ìŠ¤íƒ ëª¨ë‹ˆí„°ë§ ì‹œë‚˜ë¦¬ì˜¤"""

    # ëª¨ë“  ëª¨ë‹ˆí„°ë§ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    monitoring_manager = MonitoringManager()
    metrics_collector = MetricsCollector()
    alert_engine = AlertEngine()
    event_correlator = EventCorrelator()
    performance_monitor = PerformanceMonitor()

    # ì‹œë‚˜ë¦¬ì˜¤: ì ì§„ì  ì„±ëŠ¥ ì €í•˜ â†’ ì„ê³„ê°’ ë„ë‹¬ â†’ ì•Œë¦¼ â†’ ê·¼ë³¸ ì›ì¸ ë¶„ì„

    scenario_timeline = []

    # 1. ì •ìƒ ìƒíƒœ (T+0)
    with patch.object(metrics_collector, "collect_all_metrics") as mock_collect:
        mock_collect.return_value = {
            "timestamp": datetime.now().isoformat(),
            "status": "healthy",
            "metrics": {"cpu": 40, "memory": 60, "response_time_ms": 100},
        }

        initial_state = metrics_collector.collect_all_metrics()
        scenario_timeline.append(("T+0", "normal", initial_state))

    # 2. ì ì§„ì  ì„±ëŠ¥ ì €í•˜ ì‹œì‘ (T+5ë¶„)
    with patch.object(performance_monitor, "detect_degradation") as mock_degrade:
        mock_degrade.return_value = {
            "degradation_detected": True,
            "rate": "5% per minute",
            "affected_metric": "response_time",
            "current_value": 150,
            "baseline": 100,
        }

        degradation = performance_monitor.detect_degradation()
        scenario_timeline.append(("T+5min", "degradation_start", degradation))

    # 3. ì„ê³„ê°’ ë„ë‹¬ ë° ì•Œë¦¼ (T+10ë¶„)
    with patch.object(alert_engine, "trigger_alert") as mock_alert:
        mock_alert.return_value = {
            "alert_id": "ALT-001",
            "severity": "warning",
            "message": "Response time degraded by 50%",
            "notified": ["ops-team@company.com"],
        }

        alert = alert_engine.trigger_alert({"metric": "response_time", "value": 200, "threshold": 150})
        scenario_timeline.append(("T+10min", "alert_triggered", alert))

    # 4. ê·¼ë³¸ ì›ì¸ ë¶„ì„ (T+12ë¶„)
    with patch.object(event_correlator, "analyze_root_cause") as mock_root:
        mock_root.return_value = {
            "root_cause_found": True,
            "cause": "Database connection pool exhaustion",
            "contributing_factors": [
                "Increased user traffic (+30%)",
                "Long-running queries not optimized",
                "Connection timeout too high (30s)",
            ],
            "resolution": {
                "immediate": "Increase connection pool size",
                "long_term": "Optimize database queries",
            },
        }

        root_cause = event_correlator.analyze_root_cause(scenario_timeline)
        scenario_timeline.append(("T+12min", "root_cause_identified", root_cause))

    # 5. ìë™ ë³µêµ¬ ì‹œë„ (T+15ë¶„)
    with patch.object(monitoring_manager, "auto_remediate") as mock_remediate:
        mock_remediate.return_value = {
            "action_taken": "Increased DB connection pool from 50 to 100",
            "result": "successful",
            "metrics_after": {"response_time_ms": 120, "improvement": "40%"},
        }

        remediation = monitoring_manager.auto_remediate(root_cause)
        scenario_timeline.append(("T+15min", "auto_remediation", remediation))

    # ì‹œë‚˜ë¦¬ì˜¤ ê²€ì¦
    test_framework.assert_eq(len(scenario_timeline), 5, "Should have 5 timeline events")

    # ì„±ëŠ¥ ì €í•˜ ê°ì§€ í™•ì¸
    degradation_event = next((e for e in scenario_timeline if e[1] == "degradation_start"), None)
    test_framework.assert_ok(
        degradation_event and degradation_event[2].get("degradation_detected"),
        "Should detect performance degradation",
    )

    # ê·¼ë³¸ ì›ì¸ ë¶„ì„ í™•ì¸
    root_cause_event = next((e for e in scenario_timeline if e[1] == "root_cause_identified"), None)
    test_framework.assert_ok(
        root_cause_event and root_cause_event[2].get("root_cause_found"),
        "Should identify root cause",
    )

    # ìë™ ë³µêµ¬ í™•ì¸
    remediation_event = next((e for e in scenario_timeline if e[1] == "auto_remediation"), None)
    test_framework.assert_eq(
        remediation_event[2].get("result"),
        "successful",
        "Auto-remediation should succeed",
    )


if __name__ == "__main__":
    print("ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì‹¤ì‹œê°„ ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    os.environ["APP_MODE"] = "test"
    results = test_framework.run_all_tests()

    if results["failed"] == 0:
        print("\nâœ… ëª¨ë“  ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸ í†µê³¼!")
    else:
        print(f"\nâŒ {results['failed']}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

    sys.exit(0 if results["failed"] == 0 else 1)
